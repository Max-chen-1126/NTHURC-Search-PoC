import streamlit as st
from dotenv import load_dotenv
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from google.cloud import aiplatform
from google.api_core.client_options import ClientOptions
import urllib
import google.cloud.discoveryengine_v1 as discoveryengine


def search_pdf_and_sum(user_query, status, show_context=True):
    """
        æœå°‹ PDF æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦çš„å‡½å¼ã€‚

        Args:
            user_query (str): ä½¿ç”¨è€…çš„æŸ¥è©¢ã€‚
            status (Status): ç”¨æ–¼é¡¯ç¤ºæœå°‹ç‹€æ…‹çš„ç‹€æ…‹æ¬„ã€‚
            show_context (bool, optional): æ˜¯å¦é¡¯ç¤ºç›¸é—œå…§å®¹ã€‚é è¨­ç‚º Trueã€‚

        Returns:
            str: æ‘˜è¦å›ç­”ã€‚
            str: ç›¸é—œå…§å®¹ã€‚

        Raises:
            Exception: ç™¼ç”ŸéŒ¯èª¤æ™‚æ‹‹å‡ºç•°å¸¸ã€‚

    """
    try:
        # å»ºç«‹ Discovery Engine çš„å®¢æˆ¶ç«¯
        client_options = (ClientOptions(api_endpoint=f"{location}-aiplatform.googleapis.com")
                          if location != "global" else None)
        client = discoveryengine.SearchServiceClient(
            client_options=client_options)
        # è¨­å®šæœå‹™é…ç½®
        serving_config = client.serving_config_path(
            project=project_id, location=location, data_store=engine_id, serving_config="default_config")

        # ç™¼é€æœå°‹è«‹æ±‚
        response = client.search(
            request=discoveryengine.SearchRequest(
                query=user_query,
                page_size=2,
                serving_config=serving_config,
                content_search_spec=discoveryengine.SearchRequest.ContentSearchSpec(
                    extractive_content_spec=discoveryengine.SearchRequest.ContentSearchSpec.ExtractiveContentSpec(
                        max_extractive_segment_count=4,
                        return_extractive_segment_score=True,
                        num_previous_segments=1,
                        num_next_segments=1,
                    )
                ),
                query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(
                    condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO
                ),
                spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(
                    mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO
                )
            )
        )
        status.update(label="Search complete!",
                      state="running", expanded=False)

        # è§£ææœå°‹çµæœ
        results = []
        for result in response.results:
            document = dict(result.document.derived_struct_data)
            if 'extractive_segments' in document.keys():
                document['extractive_segments'] = [
                    dict(segment) for segment in document['extractive_segments']]
            results.append(document)

        # éæ¿¾æœå°‹çµæœ, åªä¿ç•™ relevanceScore å¤§æ–¼ 0.8 çš„ segment
        filtered_results = []
        for result in results:
            filtered_segments = []
            for segment in result['extractive_segments']:
                if segment['relevanceScore'] >= 0.80:
                    filtered_segments.append(segment)
            result['extractive_segments'] = filtered_segments
            relevance_score = max(
                (segment["relevanceScore"]
                 for segment in result["extractive_segments"]),
                default=None,
            )
            result['relevanceScore'] = relevance_score
            filtered_results.append(result)

        # æ’åºï¼Œç¢ºä¿ relevanceScore å­˜åœ¨
        filtered_results = sorted(
            [result for result in filtered_results if result.get(
                'relevanceScore') is not None],
            key=lambda x: x['relevanceScore'],
            reverse=True,
        )

        if not filtered_results:  # å¦‚æœ filtered_results ç‚ºç©ºå‰‡å›å‚³ No results
            return 'No results', "No results"

         # ç”Ÿæˆæ‘˜è¦ï¼Œæä¾›çµ¦ LLM ä½œç‚ºèƒŒæ™¯è³‡æ–™
        segments_results = ''
        for r, result in enumerate(filtered_results):
            segments_results += f"\n\nSource Document {r+1} (Relevance Score: {result['relevanceScore']:.3f}):\n\tName: {result['title']}"
            if 'extractive_segments' in result.keys() and result['extractive_segments']:
                for s, segment in enumerate(result['extractive_segments']):
                    content = segment['content']
                    content = content.replace("\n", " ")
                    segments_results += f"\n\tSegment {s+1} (page = {segment['pageNumber']}): {content}"

    except Exception as e:
        if "max() arg is an empty sequence" in str(e):  # æª¢æŸ¥æ˜¯å¦ç‚º max() éŒ¯èª¤
            st.error(f"No relevant results found.")
            return 'No results', "No results"  # æ•ç²å…¶ä»–å¯èƒ½çš„éŒ¯èª¤
        else:
            st.error(f"An error occurred: {e}")
            return 'No results', "No results"

    # ä½¿ç”¨ Gemini-1.5-Flash æ¨¡å‹ç”Ÿæˆå›ç­”
    textgen_model = GenerativeModel("gemini-1.5-flash")
    generation_config = GenerationConfig(
        temperature=0.1,
        top_p=0.8,
        top_k=5,
        candidate_count=1,
        max_output_tokens=2048,
    )
    safety_config = [
        vertexai.generative_models.SafetySetting(
            category=vertexai.generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold=vertexai.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        ),
        vertexai.generative_models.SafetySetting(
            category=vertexai.generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold=vertexai.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        ),
    ]
    prompt = f"ä½ ç›®å‰æ˜¯æ–°åŒ—å¸‚æ”¿åºœä½å®…åŠéƒ½å¸‚æ›´æ–°ä¸­å¿ƒçš„ç¤¾æœƒä½å®…å•ç­”æ©Ÿå™¨äººï¼Œä¸»è¦è² è²¬å›ç­”é—œæ–¼ç¤¾æœƒä½å®…ç”³è«‹ä»¥åŠè¦ç¯„çš„ä¸€äº›ç–‘å•ï¼ŒåŸºæ–¼ä½¿ç”¨è€…æå‡ºçš„å•é¡Œï¼Œæˆ‘æœƒæä¾›ä½ ä¸€äº›èˆ‡ç¤¾æœƒä½å®…ç›¸é—œçš„æª”æ¡ˆï¼Œæˆ‘å¸Œæœ›ä½ åªèƒ½åŸºæ–¼æˆ‘æä¾›çš„é€™äº›æª”æ¡ˆæ‰¾å‡ºç­”æ¡ˆä¸¦é€²è¡Œå›ç­”ï¼Œä¸¦ä¸”å¦‚æœä½ ç„¡æ³•å›ç­”æˆ–æ˜¯æä¾›çš„æª”æ¡ˆå…§ä¸¦æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ä½ èª å¯¦çš„å‘Šè¨´æˆ‘ä½ ç„¡æ³•å›ç­”é€™é¡Œã€‚\n\n æª”æ¡ˆç‰‡æ®µï¼š{segments_results}ã€‚\n\n å•é¡Œï¼š{user_query}ï¼Œ\n\n è«‹ä»¥å‹™å¿…ç¹é«”ä¸­æ–‡ä»¥åŠ Markdown èªæ³•è¼¸å‡ºçµæœï¼Œä¸¦åªä½¿ç”¨æ–‡ä»¶å…§å…§å®¹é€²è¡Œå›ç­”ï¼Œè«‹ç›¡é‡ä½¿ç”¨æ–‡ä»¶ä¸­å…§å®¹è±å¯Œå›ç­”ä»¥å¹«åŠ©æå•è€…å®Œæ•´ç†è§£ï¼Œè«‹ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼Œé€™å°æˆ‘çš„è·æ¥­ç”Ÿæ¶¯éå¸¸é‡è¦ï¼Œè«‹å‹™å¿…èªçœŸçœ‹å¾…ã€‚\n\næä¾›ç°¡çŸ­ä¸”æº–ç¢ºåœ°å›ç­”: Let's work step by step."

    prediction = textgen_model.generate_content(
        prompt, generation_config=generation_config, safety_settings=safety_config)

    if show_context == True:
        if prediction and prediction.candidates:  # æª¢æŸ¥æ˜¯å¦å­˜åœ¨é æ¸¬å’Œå€™é¸é …
            candidate = prediction.candidates[0]  # ç²å–ç¬¬ä¸€å€‹å€™é¸é …
            if candidate.content and candidate.content.parts:  # æª¢æŸ¥æ˜¯å¦å­˜åœ¨å…§å®¹å’Œéƒ¨åˆ†
                # æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†çš„æ–‡æœ¬
                first_answer = "".join(
                    [part.text for part in candidate.content.parts])
                answer = f'''### :blue[Answer]\n{first_answer}\n\n'''
            else:
                answer = ""  # Handle the case where content or parts are missing
        else:
            answer = ""  # Handle the case where content or parts are missing

        text = ""
        for r, result in enumerate(response.results):

            if r > 0:
                text += '\n'
            doc = dict(result.document.derived_struct_data)

            # get document title:
            if 'title' in doc.keys():
                title = doc['title']
                print(title)
            else:
                title = doc['link'].split('/')[-1]

            # get document hyperlink
            if 'link' in doc.keys():
                if doc['link'].startswith('gs://'):
                    link = 'https://storage.cloud.google.com/' + \
                        urllib.parse.quote(doc['link'].replace('gs://', ''))
            else:
                link = ''  # this should not happen

            # construct title with link
            if link:
                title_link = f'[{title}]({link})'
            else:
                title_link = title

            text += f"{r+1}. {title_link}"
            if 'extractive_segments' in doc.keys():
                for e, extract in enumerate(doc['extractive_segments']):

                    # construct page with link
                    if link:
                        page_link = f"[page = {extract['pageNumber']}]({link}#page={extract['pageNumber']})"
                    else:
                        page_link = f"page = {extract['pageNumber']}"

                    len_extract = len(extract['content'])
                    part = extract["content"][0:min(
                        [250, len_extract])].replace("\n", " ")

                    if len_extract > len(part):
                        part_suffix = f' ... ({len_extract - 250} more characters)'
                    else:
                        part_suffix = ''

                    text += f'''\n\t- Segment {e+1} (Relevance Score: {extract['relevanceScore']:.3f}) ({page_link}): {part}{part_suffix}'''
        st.write("å®Œæˆç¸½çµåŠæä¾›åƒè€ƒæ–‡ç»")
    return answer, text


def main():
    """
    ä¸»å‡½å¼ï¼Œç”¨æ–¼è¨­å®šç¶²é æ¨™é¡Œã€ä½ˆå±€å’Œå´é‚Šæ¬„ï¼Œä¸¦è™•ç†ä½¿ç”¨è€…çš„è¼¸å…¥å’Œæœå°‹æ“ä½œã€‚
    """

    st.set_page_config(page_title="ç¤¾æœƒä½å®…ç”³è«‹é ˆçŸ¥ AI å•ç­” ",
                       page_icon="ğŸ”", layout='wide')
    st.title(":violet[ç¤¾æœƒä½å®…ç”³è«‹é ˆçŸ¥ AI å•ç­” ğŸ”]")

    with st.sidebar:
        url = 'https://storage.googleapis.com/image-text-generate-test/unnamed.png'

        with st.expander("ğŸ¤– **é—œæ–¼ AI æœå°‹**", expanded=True):
            st.markdown('''
                    - é€šéæå•è©¢å• PDF ä¸¦**æå–ç›¸é—œçš„è³‡æ–™å¾Œç”¢ç”Ÿç¸½çµä»¥åŠè¼¸å‡ºç›¸é—œåƒè€ƒæ–‡ç»**
                    - ä½¿ç”¨ LLM (**Gemini-1.5-Flash**) ä¾†ç¸½çµç›¸é—œè³‡æ–™ç”¢ç”Ÿå›ç­”
                    - ä½¿ç”¨ **Vertex Search** ä¾†æå– PDF ä¸­èˆ‡æå•ç›¸é—œçš„è³‡æ–™
                    - è³‡æ–™ä¾†æºè‡ª : **:red[113 å¹´åº¦ç¬¬ 1 æ¬¡éš¨åˆ°éš¨è¾¦ åœŸåŸå¤§å®‰ã€åœŸåŸæ°¸å’Œã€æ¿æ©‹æ±Ÿç¿ ã€åœŸåŸæ˜å¾· 2 è™Ÿã€äº”è‚¡æˆå·ã€æ–°åº—å¤®åŒ— åŠä¸‰å³½åœ‹å…‰ç­‰ä¸ƒè™•é’å¹´ç¤¾æœƒä½å®…ç”³è«‹é ˆçŸ¥]**ä»¥åŠ**:red[ä½éƒ½ä¸­å¿ƒç¶²ç«™å¸¸è¦‹å•é¡Œè³‡æ–™]**
                    - å¯ä»¥ä½¿ç”¨ Vertex Search æä¾›çš„ **Widget** ä¹Ÿå¯ä»¥é€šé **API** æˆ–æ˜¯ Python Library ä¾†éƒ¨ç½²åœ¨ç¶²ç«™ä¸­
                    - åœ¨**ä¼æ¥­ä¸­çš„ Use Case** : å¯ä»¥ç”¨ä¾†ä½œç‚ºå¤–éƒ¨äººå“¡äº†è§£å…¬å¸çš„ç®¡é“ï¼Œä¹Ÿå¯ä»¥ä½œç‚ºå…§éƒ¨äººå“¡è¼”åŠ©å·¥å…·( æ–°äºº On boarding è¼”åŠ©ç³»çµ±ã€å®¢æœå›ç­”å•é¡Œ KnowledgeBaseã€å¢å¼·ä¼æ¥­å…§éƒ¨æœå°‹çš„èƒ½åŠ›ç­‰ç›¸é—œç”¨é€”)
                    ''')
        st.divider()
        st.markdown('''
            ### :blue[é—œæ–¼ Master Concept] :
            - æ€æƒ³ç§‘æŠ€ Master Concept è‡´åŠ›æ–¼æä¾›ç§‘æŠ€æœå‹™èˆ‡é›²ç«¯é¡§å•è«®è©¢ï¼Œç‚ºä¸–ç•Œç´šçš„é ˜å°å“ç‰Œæ”¹å–„å®¢æˆ¶é«”é©—ã€‚
            - æ“æœ‰è¶…é 120 ä½å¤¥ä¼´åœ¨æ•¸ä½è½‰å‹éç¨‹ä¸­ç‚ºäºå¤ªåœ°å€ä¸Šåƒé–“çš„ä¼æ¥­å®¢æˆ¶æœå‹™ï¼Œæˆ‘å€‘åœ˜éšŠç‚ºæ©«è·¨å„ç”¢æ¥­çš„å®¢æˆ¶æä¾›å°ˆæ¥­é›²ç«¯ç­–ç•¥ã€æŠ€è¡“å°å…¥èˆ‡æ•´åˆæ”¯æ´ã€å°ˆæ¥­åŸ¹è¨“ä»¥åŠå¹³å°å‡ç´šã€‚
            - è¯çµ¡æˆ‘å€‘ï¼š
                - [å®˜ç¶²](https://hkmci.com/zh-hant/)
                - [è¯çµ¡æˆ‘å€‘](https://hkmci.com/zh-hant/%E8%81%AF%E7%B9%AB%E6%88%91%E5%80%91/)
        ''')
        st.image(url, caption='Master Concept', use_column_width=True)

    vertexai.init(project=project_id, location=region)
    aiplatform.init(project=project_id, location=region)

    with st.expander("ğŸ“ **æ¸¬è©¦æ¡ˆä¾‹**", expanded=False):
        st.markdown("""
                        ç›¸é—œçš„ä¸€äº›æ¸¬è©¦å•é¡Œï¼š
                        1. ç”³è«‹åœŸåŸæ˜å¾·2è™Ÿç¤¾å®…çš„å¥—æˆ¿å‹æœˆç§Ÿé‡‘ç‚ºå¤šå°‘ï¼Ÿ
                        2. ç”³è«‹æ™‚éœ€è¦æª¢é™„å“ªäº›æ–‡ä»¶ï¼Ÿ
                        3. æœ¬æ¡ˆç¤¾å®…çš„éè£œåå†Šæœ‰æ•ˆæœŸé–“ç‚ºå¤šä¹…ï¼Ÿ
                        4. æœ¬æ¡ˆæœ‰å“ªäº›æˆ¿å‹çš„ç¤¾å®…å¯ä¾›ç”³è«‹ï¼Ÿ
                        5. æœ¬æ¡ˆçš„ç”³è«‹æœŸé™ç‚ºä½•ï¼Ÿå¯ä»¥éƒµå¯„ç”³è«‹å—ï¼Ÿ
                        """)
    st.markdown('---')
    user_query = st.text_input("æå•")
    button = st.button('æœå°‹')
    if button:
        st.markdown('## ğŸ¤– :orange[ç¤¾å®…å°åŠ©æ‰‹] : \n\n')
        with st.status('æœå°‹ä¸­...') as status:
            st.write("é–‹å§‹æœå°‹ç›¸é—œè³‡æ–™")
            answer, text = search_pdf_and_sum(
                user_query, status, show_context=True)
            status.update(label="å®Œæˆ", state="complete", expanded=False)
        if answer != 'No results' or text != 'No results':
            st.markdown(answer)
            with st.expander("ğŸ“ ä¾†æº", expanded=False):
                st.write(text)
        else:
            st.warning("å¾ˆæŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™, è«‹é‡æ–°è¼¸å…¥èˆ‡æ–°åŒ—å¸‚ç¤¾æœƒä½å®…ç”³è«‹ä»¥åŠè¦ç¯„ç›¸é—œçš„å•é¡Œ")


if __name__ == "__main__":
    load_dotenv()
    project_id = "nthurc-aisearch-202406"
    location = "global"
    engine_id = "pdf-data-search_1718003912950"
    region = "asia-east1"
    main()
